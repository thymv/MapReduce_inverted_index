// Thy Vu
// CSS 534 Fall 2016
// Professor Munehiro Fukuda
// Program Assignment 3: MapReduce

import java.io.IOException;
import java.util.*;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.util.*;


public class AAAAA{

    public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, Text> {	 
        JobConf conf;
        public void configure( JobConf job ) {
            this.conf = job;
        }
        
        public void map(LongWritable docId, Text value, OutputCollector<Text, Text> output,
                     Reporter reporter) throws IOException {
            // retrieve # keywords from JobConf
            int argc = Integer.parseInt( conf.get( "argc" ) );
            
            // put keywords into array
            String[] keywordArr= new String[argc];
            int keywordCount[] = new int[argc];
            for (int i = 0; i < argc; i++) {
                keywordArr[i] = conf.get("keyword"+i);

            }
  
            // get the current file name
            FileSplit fileSplit = ( FileSplit )reporter.getInputSplit();
            String filename = "" + fileSplit.getPath().getName();
            
            // check each word from input, if it matches a keyword, increase count for keyword
            String line = value.toString();
	        StringTokenizer tokenizer = new StringTokenizer(line);
	        while (tokenizer.hasMoreTokens()) {
		        String word = new String(tokenizer.nextToken());
		        for (int i = 0; i < argc; i++){
		            if (word.equals(keywordArr[i])){
		                keywordCount[i] = keywordCount[i]+1;
		                break;
		            }
		        }
		    }
            
            // generate and pass (keyword, filename_count) pair to Reduce
            for (int i = 0; i< argc; i++){
                if(keywordCount[i] > 0){
                    output.collect(new Text(keywordArr[i]), new Text(filename+"_"+keywordCount[i])); 
                }
            }

        }
    } 
    
    
    public static class Reduce extends MapReduceBase implements Reducer<Text, Text, Text, Text> {
        public void reduce(Text key, Iterator<Text> values, OutputCollector<Text, Text> output,
                            Reporter reporter) throws IOException {
            
            // LinkedList store filenames
            LinkedList<String> filenameList = new LinkedList<String>();   
            // LinkedList to store counts for corresponding filenames of same index
            LinkedList<Integer> countList = new LinkedList<Integer>();  
            
            // for each filename_count, add to document name list if filename is
            // not on the list. If it already exists on list, update the count.
            while(values.hasNext()){
            
                // get "filename_count" then extract "filename" and "count"
                String filenameCount = values.next().toString();
                int indexOfCount = filenameCount.lastIndexOf('_') +1;
                String filename= filenameCount.substring(0, indexOfCount-1);
                String countStr = filenameCount.substring(indexOfCount);
                if (countStr.charAt(countStr.length()-1) == ' '){ 
                    // take care of extra space character generated by hadoop
                    countStr = countStr.substring(0, countStr.length()-1);
                }
                Integer countInteger = new Integer(countStr);
                
                // if list doesn't have this filename yet, add this filename and its count
                // to lists. Else, sum up and update the count.
                int indexOnFilenameList = filenameList.indexOf(filename);
                if (indexOnFilenameList == -1){
                    filenameList.add(filename);
                    countList.add(countInteger);
                }else{
                    int oldCount = countList.remove(indexOnFilenameList).intValue();
                    countList.add(indexOnFilenameList, new Integer(oldCount + countInteger.intValue()));
                }
            } //end while loop 
            
            
            
            //****** ADDITIONAL WORK: sort filenames from most to least count **********
            
            // orderedFileName contains filenames from most to least counts
            LinkedList<String> orderedFilename = new LinkedList<String>();
            // orderedCount contains counts for corresponding filenames of same index 
            LinkedList<Integer> orderedCount = new LinkedList<Integer>();
            
            // iterators for the (unordered) filename and count lists
            Iterator<String> filenameIt = filenameList.iterator();
            Iterator<Integer> countIt = countList.iterator();
            
            // add first file & count into ordered lists
            orderedFilename.add(filenameIt.next());
            orderedCount.add(countIt.next());

            // add all files and counts to ordered lists (Most count to least count)
            while(filenameIt.hasNext() && countIt.hasNext() ){
                String fileToAdd = filenameIt.next();
                Integer countOfFileToAdd = countIt.next();
                Iterator<Integer> orderedCountIt = orderedCount.iterator();
                int indexToAddFile = 0;
                
                // find appropriate place on ordered lists
                while (orderedCountIt.hasNext()){
                    //Integer currentCount = new Integer(orderedCountIt.next().intValue());
                    if (countOfFileToAdd.compareTo(orderedCountIt.next()) < 0){ 
                        // nextCount <  current, go to next on ordered list
                        indexToAddFile++;
                    }
                } 
                
                // add file and count at appropriate index
                orderedFilename.add(indexToAddFile, fileToAdd);
                orderedCount.add(indexToAddFile, countOfFileToAdd);
            }
            
            
            // ***************    END ADDITIONAL WORK     *****************
            
            
            // concatenate all filename_counts into docListText
            String docListText = new String();
            Iterator filenameIt2 = orderedFilename.iterator();
            Iterator countIt2 = orderedCount.iterator();
            while(filenameIt2.hasNext() && countIt2.hasNext()){
                docListText = docListText + filenameIt2.next() +"_"+ countIt2.next().toString()+ " ";
            }
            
            // print list of filename_counts for a keyword
            output.collect(key, new Text(docListText) );
        }
    }


    public static void main(String[] args) throws Exception {
        // input format:
        // hadoop jar invertedindexes.jar InvertedIndexes input output keyword1 keyword2 ...
        JobConf conf = new JobConf(AAAAA.class); // AAAAA is this programâ€™s file name

        conf.setJobName("BBBBB"); // BBBBB is a job name, whatever you like
        conf.setOutputKeyClass(Text.class);
        conf.setOutputValueClass(Text.class);
        conf.setMapperClass(Map.class);
        conf.setCombinerClass(Reduce.class);
        conf.setReducerClass(Reduce.class);
        conf.setInputFormat(TextInputFormat.class);
        conf.setOutputFormat(TextOutputFormat.class);
        FileInputFormat.setInputPaths(conf, new Path(args[0])); // input directory name
        FileOutputFormat.setOutputPath(conf, new Path(args[1])); // output directory name
        conf.set( "argc", String.valueOf( args.length - 2 ) ); // argc maintains #keywords
        for ( int i = 0; i < args.length - 2; i++ )
            conf.set( "keyword" + i, args[i + 2] ); // keyword1, keyword2, ...
            
        long startTime = System.currentTimeMillis();
        long endTime;
        
        JobClient.runJob(conf);
        
        endTime = System.currentTimeMillis();
	    System.out.println("****************************");
	    System.out.println("Execution took " + (endTime-startTime)+" miliseconds.");
	    System.out.println("****************************");
 
    }

}